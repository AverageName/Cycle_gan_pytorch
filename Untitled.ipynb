{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_loss(G, D, X, Y):\n",
    "    return torch.sum(torch.pow((D(Y) - 1), 2) + torch.pow(D(G(X)), 2)).item()\n",
    "\n",
    "\n",
    "def G_loss(G, D, X, Y):\n",
    "    return torch.pow((D(G(X)) - 1), 2).item()\n",
    "\n",
    "\n",
    "def CC_loss(F, G, X, Y):\n",
    "    return torch.sum(torch.abs(F(G(X)) - X)).item() + torch.sum(torch.abs(G(F(Y)) - Y)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_module(name):\n",
    "    if name == \"batch\":\n",
    "        return nn.BatchNorm2d\n",
    "    elif name == \"instance\":\n",
    "        return nn.InstanceNorm2d\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNormRelu(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=(1, \"zeros\"),\n",
    "                 stride=1, norm=\"batch\", leaky=True, conv_type=\"forward\"):\n",
    "        super(ConvNormRelu, self).__init__()\n",
    "        if padding[1] == \"zeros\":\n",
    "            self.pad = None\n",
    "            if conv_type == \"forward\":\n",
    "                self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                      kernel_size=kernel_size, stride=stride, padding=padding[0])\n",
    "            elif conv_type == \"transpose\":\n",
    "                self.conv = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                      kernel_size=kernel_size, stride=stride, padding=padding[0], output_padding=padding[0])\n",
    "        elif padding[1] == \"reflection\":\n",
    "            if conv_type == \"forward\":\n",
    "                self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                      kernel_size=kernel_size, stride=stride)\n",
    "                self.pad = nn.ReflectionPad2d(padding[0])\n",
    "            elif conv_type == \"transpose\":\n",
    "                self.conv = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                      kernel_size=kernel_size, stride=stride, padding=padding[0], output_padding=padding[0])\n",
    "                self.pad = None\n",
    "            \n",
    "            \n",
    "        self.leaky = leaky\n",
    "        if norm:\n",
    "            self.norm = get_norm_module(norm)(out_channels)\n",
    "        else:\n",
    "            self.norm = None\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = inputs\n",
    "        if self.pad is not None:\n",
    "            out = self.pad(out)\n",
    "        #print(out.shape)\n",
    "        out = self.conv(out)\n",
    "        #print(out.shape)\n",
    "        if self.norm is not None:\n",
    "            out = self.norm(out)\n",
    "        if self.leaky:\n",
    "            return F.leaky_relu(out, negative_slope=0.2)\n",
    "        else:\n",
    "            return F.relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif type(m) == nn.InstanceNorm2d:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.normal_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure out real PatchGan\n",
    "class PatchGan(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels):\n",
    "        super(PatchGan, self).__init__()\n",
    "        \n",
    "        self.layer1 = ConvNormRelu(in_channels=input_channels, out_channels=64, kernel_size=4,\n",
    "                                        padding=(1, \"zeros\"), stride=2, norm=None)\n",
    "        self.layer2 = ConvNormRelu(in_channels=64, out_channels=128, kernel_size=4,\n",
    "                                        padding=(1, \"zeros\"), stride=2, norm=\"instance\")\n",
    "        self.layer3 = ConvNormRelu(in_channels=128, out_channels=256, kernel_size=4,\n",
    "                                        padding=(1, \"zeros\"), stride=2, norm=\"instance\")\n",
    "        #self.layer4 = ConvBatchNormRelu(in_channels=256, out_channels=512, kernel_size=4,\n",
    "         #                               padding=1, stride=2, batch_norm=True)\n",
    "        self.layer4 = ConvNormRelu(in_channels=256, out_channels=512, kernel_size=4,\n",
    "                                        padding=(1, \"zeros\"), stride=1, norm=\"instance\")\n",
    "        \n",
    "        self.conv_fc = nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4,\n",
    "                                 padding=1, stride=1)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        out = self.layer1(inputs)\n",
    "        #print(out.shape)\n",
    "        out = self.layer2(out)\n",
    "        #print(out.shape)\n",
    "        out = self.layer3(out)\n",
    "        #print(out.shape)\n",
    "        out = self.layer4(out)\n",
    "        #print(out.shape)\n",
    "        out = self.conv_fc(out)\n",
    "        #print(out.shape)\n",
    "        return F.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 140, 140])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.rand((1, 64, 70, 70))\n",
    "#model = PatchGan(64)\n",
    "layer = ConvNormRelu(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=(1, \"reflection\"),\n",
    "                     conv_type=\"transpose\")\n",
    "layer(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_planes, norm=\"batch\"):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.pad1 = nn.ReflectionPad2d(1)\n",
    "        self.pad2 = nn.ReflectionPad2d(1)\n",
    "        self.norm1 = get_norm_module(norm)(in_planes)\n",
    "        self.norm2 = get_norm_module(norm)(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_planes, out_channels=in_planes, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=in_planes, out_channels=in_planes, kernel_size=3)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = self.conv1(self.pad1(inputs))\n",
    "        out = F.relu(self.norm1(out))\n",
    "        out = self.conv2(self.pad2(out))\n",
    "        out = self.norm2(out)\n",
    "        return out + inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, n_blocks):\n",
    "        super(ResnetGenerator, self).__init__()\n",
    "        \n",
    "        self.conv1 = ConvNormRelu(in_channels=in_channels, out_channels=64, kernel_size=7,\n",
    "                                       padding=(3, \"reflection\"), stride=1, norm=\"instance\", leaky=False)\n",
    "        self.conv2 = ConvNormRelu(in_channels=64, out_channels=128, kernel_size=3,\n",
    "                                  padding=(1, \"reflection\"), stride=2, norm=\"instance\", leaky=False)\n",
    "        self.conv3 = ConvNormRelu(in_channels=128, out_channels=256, kernel_size=3,\n",
    "                                  padding=(1, \"reflection\"), stride=2, norm=\"instance\", leaky=False)\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(n_blocks):\n",
    "            self.blocks.append(ConvNormRelu(in_channels=256, out_channels=256, kernel_size=3,\n",
    "                                            padding=(1, \"reflection\"), stride=1, norm=\"instance\", leaky=False))\n",
    "        self.conv4 = ConvNormRelu(in_channels=256, out_channels=128, kernel_size=3, \n",
    "                                  padding=(1, \"reflection\"), stride=2, norm=\"instance\", leaky=False, conv_type=\"transpose\")\n",
    "        self.conv5 = ConvNormRelu(in_channels=128, out_channels=64, kernel_size=3, \n",
    "                                  padding=(1, \"reflection\"), stride=2, norm=\"instance\", leaky=False, conv_type=\"transpose\")\n",
    "        self.conv6 = ConvNormRelu(in_channels=64, out_channels=3, kernel_size=7, \n",
    "                                  padding=(3, \"reflection\"), stride=1, norm=\"instance\", leaky=False)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = self.conv1(inputs)\n",
    "        #print(out.shape)\n",
    "        out = self.conv2(out)\n",
    "        #print(out.shape)\n",
    "        out = self.conv3(out)\n",
    "        #print(out.shape)\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "            #print(out.shape)\n",
    "        out = self.conv4(out)\n",
    "        #print(out.shape)\n",
    "        out = self.conv5(out)\n",
    "        #print(out.shape)\n",
    "        out = self.conv6(out)\n",
    "        #print(out.shape)\n",
    "        return F.tanh(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = ConvNormRelu(in_channels=48, out_channels=128, kernel_size=3, \n",
    "                                  padding=(1, \"reflection\"), stride=2, norm=\"instance\", leaky=False, conv_type=\"transpose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "data = torch.rand((1, 48, 256, 256))\n",
    "print(layer(data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_optimize_step(G1, G2, D2, inputs):\n",
    "    optimizer = optim.Adam(G1.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    discr_output = D2(G1(inputs))\n",
    "    adv_loss = F.mse_loss(discr_output, torch.ones(discr_output.shape).cuda())\n",
    "    identity_loss = F.l1_loss(G2(inputs), inputs)\n",
    "    fwd_cycle_loss = F.l1_loss(G2(G1(inputs)), inputs)\n",
    "    bwd_cycle_loss = F.l1_loss(G1(G2(inputs)), inputs)\n",
    "    loss = adv_loss + 5 * identity_loss + 10 * (fwd_cycle_loss + bwd_cycle_loss)\n",
    "    print(\"Gen loss: \", loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_optimize_step(G, D, inputs_domain, other_domain):\n",
    "    optimizer = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    discr_output = D(inputs_domain)\n",
    "    d_loss = F.mse_loss(discr_output, torch.ones(discr_output.shape).cuda())\n",
    "    gen_output = G(other_domain)\n",
    "    g_loss = F.mse_loss(gen_output, torch.zeros(gen_output.shape).cuda())\n",
    "    loss = d_loss + g_loss\n",
    "    print(\"Discr loss: \", loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Apple2OrangeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root, folder_names, transform=None):\n",
    "        super(Apple2OrangeDataset, self).__init__()\n",
    "        \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.folder_names = folder_names\n",
    "        self.A_size = len(os.listdir(os.path.join(root, folder_names[0])))\n",
    "        self.B_size = len(os.listdir(os.path.join(root, folder_names[1])))\n",
    "        self.A_paths = sorted(os.listdir(os.path.join(root, folder_names[0])))\n",
    "        self.B_paths = sorted(os.listdir(os.path.join(root, folder_names[1])))\n",
    "        #print(self.A_paths)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return max(self.A_size, self.B_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx_A = idx % self.A_size\n",
    "        idx_B = idx % self.B_size\n",
    "        image_A = Image.open(os.path.join(self.root, self.folder_names[0], self.A_paths[idx_A]))\n",
    "        image_B = Image.open(os.path.join(self.root, self.folder_names[1], self.B_paths[idx_B]))\n",
    "        if self.transform is not None:\n",
    "            image_A = self.transform(image_A)\n",
    "            image_B = self.transform(image_B)\n",
    "            \n",
    "        return {\"A\": image_A, \"B\": image_B}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Apple2OrangeDataset('/home/dpakhom1/Cycle_gan_pytorch/datasets/apple2orange/',\n",
    "                              [\"trainA\", \"trainB\"], transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=1, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(num_epochs, dataloader, G1, G2, D1, D2):\n",
    "    for epoch in range(num_epochs):\n",
    "        for idx, data in enumerate(dataloader):\n",
    "            domain_A, domain_B = data[\"A\"].cuda(), data[\"B\"].cuda()\n",
    "            \n",
    "            generator_optimize_step(G1, G2, D2, domain_A)\n",
    "            discriminator_optimize_step(G2, D2, domain_A, domain_B)\n",
    "            \n",
    "            generator_optimize_step(G2, G1, D1, domain_B)\n",
    "            discriminator_optimize_step(G1, D1, domain_B, domain_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1 = ResnetGenerator(3, 6)\n",
    "G2 = ResnetGenerator(3, 6)\n",
    "D1 = PatchGan(3)\n",
    "D2 = PatchGan(3)\n",
    "G1 = G1.cuda()\n",
    "G2 = G2.cuda()\n",
    "D1 = D1.cuda()\n",
    "D2 = D2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dpakhom1/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/dpakhom1/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss:  tensor(35.9632, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.3944, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(29.3419, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.4640, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(33.9537, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.3581, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(41.8624, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.3673, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(35.9180, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.3971, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(28.3024, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2872, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(29.6631, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2265, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(31.3848, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2352, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(40.5285, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2288, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(30.2821, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1928, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(46.3674, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1929, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(30.0043, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1855, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(36.3092, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1943, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(26.7128, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1719, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(35.4004, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1942, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(29.4063, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1806, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(17.8650, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1804, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(19.7746, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1818, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(33.0248, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1739, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(31.3904, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1727, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(27.0335, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1527, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(32.6662, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1369, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(36.3924, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1379, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(35.8382, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1020, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(37.1259, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1262, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(27.1879, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1715, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(32.2959, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1336, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(23.5116, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1319, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(27.3989, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1266, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(20.3054, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.0913, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(37.7594, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1165, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(23.1376, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1343, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(34.9470, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1452, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(32.5344, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.0905, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(35.5592, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1212, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(27.6995, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1574, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(36.1776, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1426, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(25.2353, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1665, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(25.1308, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1404, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(34.5665, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.0918, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(34.9976, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.0681, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(24.1827, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1578, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(26.5362, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1213, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(28.4106, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1668, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(26.7498, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1485, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(28.3760, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1031, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(39.4332, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1225, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(33.0057, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1863, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(28.5510, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1430, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(36.8313, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.0991, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(34.7204, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1318, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(22.9988, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1146, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(21.4516, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1680, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(37.5063, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1710, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(25.2149, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1823, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(45.4110, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1580, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(27.9488, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1803, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(28.1141, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1443, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(37.1082, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2054, device='cuda:0', grad_fn=<ThAddBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss:  tensor(39.9617, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.0992, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(30.4257, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1874, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(32.8419, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2067, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(23.5778, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1603, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(23.4507, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1531, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(22.4053, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1485, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(35.6005, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1448, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(44.1465, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.0778, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(32.0157, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1817, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(33.4530, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1445, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(28.2923, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2140, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(20.3746, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1911, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(35.0900, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1382, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(25.1983, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2188, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(38.2322, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1666, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(31.6964, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1631, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(17.6522, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2292, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(25.4565, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2107, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(19.8118, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1486, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(29.4954, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1918, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(30.0612, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2182, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(26.9829, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2030, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(25.4466, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2096, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(31.5266, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1986, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(23.7032, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1877, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(25.3867, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2220, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(29.1249, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2157, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(30.4729, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2106, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(22.8353, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2076, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(20.4034, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2389, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(40.9070, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2240, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(35.2874, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2477, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(36.7953, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2534, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(35.7912, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1862, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(19.4205, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2347, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(32.0550, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2407, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(36.7378, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2178, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(22.3719, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1459, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(24.8006, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1601, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(23.1939, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1725, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(40.2498, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1629, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(21.7754, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1893, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(26.9248, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1367, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(23.4364, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1742, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(29.8221, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1483, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(36.8408, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1435, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(21.6712, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1618, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(38.7131, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1375, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(28.7591, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2426, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(21.8792, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1404, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(24.5372, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2220, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(46.9521, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1614, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(29.7323, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2086, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(22.9370, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1921, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(31.7611, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2315, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(29.0149, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1920, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(23.4165, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2544, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(34.3078, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1707, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(18.2910, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1828, device='cuda:0', grad_fn=<ThAddBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss:  tensor(35.8197, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1207, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(21.0387, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1320, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(24.5102, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1545, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(22.5906, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1467, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(28.8967, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1083, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(30.6214, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1709, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(22.2586, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.0723, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(25.7901, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2414, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(19.5162, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.0998, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(30.0323, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1744, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(27.0565, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1607, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(22.1904, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2191, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(23.1517, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1765, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(15.7513, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1976, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(37.8631, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1963, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(24.6730, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2492, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(32.8843, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1716, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(30.2758, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2560, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(26.0991, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2168, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(26.3605, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1795, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(19.9767, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1772, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(15.5233, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1906, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(31.4861, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1628, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(28.1410, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1635, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(23.4418, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1278, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(24.6243, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1988, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(37.0568, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1014, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(25.8113, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1294, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(30.6326, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2058, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(19.4058, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1416, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(28.3088, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1259, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(22.2137, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2283, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(41.0580, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1758, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(22.8315, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.0670, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(28.5490, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1447, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(26.9033, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2068, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(36.2943, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1589, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(15.4985, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2278, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(29.6551, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1576, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(16.0408, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1466, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(18.6369, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1394, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(20.0301, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1455, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(20.8105, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1419, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(23.2375, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1492, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(35.0653, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2055, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(43.3718, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2303, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(19.3903, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1566, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(24.9819, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1959, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(25.9931, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1861, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(26.0145, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1651, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(20.4156, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1588, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(31.4946, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1094, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(25.0533, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1433, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(14.9413, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1487, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(36.2347, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1664, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(17.9221, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1492, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(42.0565, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2127, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(35.7632, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2395, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(32.6454, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1354, device='cuda:0', grad_fn=<ThAddBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss:  tensor(30.0256, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2469, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(20.7609, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1744, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(21.8634, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1653, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(23.1824, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1556, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(31.9478, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1946, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(24.5791, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2419, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(34.9248, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1720, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(26.1203, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2024, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(21.1625, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1472, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(30.2504, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1438, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(23.2613, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1132, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(31.3895, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1431, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(23.8930, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1655, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(28.4827, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1322, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(23.3780, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2127, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(36.7624, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2067, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(25.5893, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2493, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(32.2670, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2328, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(27.6347, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1680, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(22.7458, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2211, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(20.7819, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2440, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(21.3655, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1668, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(30.5064, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1670, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(20.7076, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1554, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(19.9187, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2161, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(34.0088, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1710, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(34.5733, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2447, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(35.9226, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2373, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(29.1772, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2471, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(34.6683, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2041, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(29.6008, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1614, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(26.7280, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1577, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(16.5597, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2507, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(27.1988, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2291, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(16.7322, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1720, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(28.0441, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2455, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(29.0452, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2285, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(23.0835, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1850, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(18.0549, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2403, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(34.9914, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2240, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(22.5754, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2217, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(34.9772, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2435, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(31.0964, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2706, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(23.3258, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2330, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(36.4426, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2552, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(15.8368, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2340, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(22.2973, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2498, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(27.8822, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1985, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(26.9063, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2609, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(18.6209, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1585, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(30.2657, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1171, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(23.5583, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1654, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(17.4504, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1656, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(33.4345, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1552, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(23.6082, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2596, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(32.9423, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2329, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(35.6388, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1550, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(32.6609, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1801, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(30.5647, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.2627, device='cuda:0', grad_fn=<ThAddBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss:  tensor(25.8936, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1634, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Gen loss:  tensor(22.5105, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Discr loss:  tensor(0.1395, device='cuda:0', grad_fn=<ThAddBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dpakhom1/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dpakhom1/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dpakhom1/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dpakhom1/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dpakhom1/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/dpakhom1/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/dpakhom1/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/dpakhom1/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/dpakhom1/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/dpakhom1/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/dpakhom1/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/dpakhom1/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/dpakhom1/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 920, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/dpakhom1/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 920, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/dpakhom1/anaconda3/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "  File \"/home/dpakhom1/anaconda3/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss:  tensor(30.3478, device='cuda:0', grad_fn=<ThAddBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _DataLoaderIter.__del__ at 0x7f45a0f2ad90>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dpakhom1/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 397, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/home/dpakhom1/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 116756) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6bbf581fb104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-b230af8ef068>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(num_epochs, dataloader, G1, G2, D1, D2)\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mdomain_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"A\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mgenerator_optimize_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mdiscriminator_optimize_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-da5c6c3da910>\u001b[0m in \u001b[0;36mgenerator_optimize_step\u001b[0;34m(G1, G2, D2, inputs)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madv_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0midentity_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfwd_cycle_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbwd_cycle_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gen loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loop(1, dataloader, G1, G2, D1, D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
