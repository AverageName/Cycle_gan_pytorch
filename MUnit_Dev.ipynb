{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from models.CycleGAN import *\n",
    "from datasets.UnalignedDataset import UnalignedDataset\n",
    "from utils.utils import ImageBuffer, set_requires_grad, tensor_to_image, save_cyclegan_model, get_activation, get_norm_module\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_channels, num_blocks, content_dim):\n",
    "        super(ContentEncoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = ConvNormRelu(in_channels=num_channels, out_channels=64,\n",
    "                                  kernel_size=7, padding=(3, \"zeros\"), leaky=False, norm='instance')\n",
    "        self.conv2 = ConvNormRelu(in_channels=64, out_channels=128, \n",
    "                                  kernel_size=4, padding=(1, \"zeros\"), stride=2, leaky=False, norm='instance')\n",
    "        self.conv3 = ConvNormRelu(in_channels=128, out_channels=256, \n",
    "                                 kernel_size=4, padding=(1, \"zeros\"), stride=2, leaky=False, norm='instance')\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for _ in range(num_blocks):\n",
    "            self.blocks.append(ResBlock(in_planes=content_dim, kernel_size=3, padding=(1, \"reflection\"), norm=\"instance\"))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        out = self.conv1(inputs)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_channels, style_dims):\n",
    "        super(StyleEncoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = ConvNormRelu(in_channels=num_channels, out_channels=64,\n",
    "                                  kernel_size=7, padding=(3, \"zeros\"), leaky=False, norm='instance')\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        dims = 64\n",
    "        prev_dims = 0\n",
    "        n_convs = 4\n",
    "        \n",
    "        for _ in range(n_convs):\n",
    "            prev_dims = dims\n",
    "            dims = min(dims * 2, 256)\n",
    "            \n",
    "            self.convs.append(ConvNormRelu(in_channels=prev_dims, out_channels=dims, \n",
    "                                  kernel_size=4, padding=(1, \"zeros\"), stride=2, leaky=False, norm='instance'))\n",
    "        \n",
    "        self.conv_fc = nn.Conv2d(dims, style_dims, kernel_size=1, stride=1, padding=0)  \n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        out = self.conv1(inputs)\n",
    "        for conv in self.convs:\n",
    "            out = conv(out)\n",
    "            \n",
    "        #Fastest version of Global Average Pooling\n",
    "        out = torch.mean(out.view(out.size(0), out.size(1), -1), dim=2)\n",
    "        out = self.conv_fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nvidia implementation\n",
    "class AdaptiveInstanceNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super(AdaptiveInstanceNorm2d, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        # weight and bias are dynamically assigned\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        # just dummy buffers, not used\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.weight is not None and self.bias is not None, \"Please assign weight and bias before calling AdaIN!\"\n",
    "        b, c = x.size(0), x.size(1)\n",
    "        running_mean = self.running_mean.repeat(b)\n",
    "        running_var = self.running_var.repeat(b)\n",
    "\n",
    "        # Apply instance norm\n",
    "        x_reshaped = x.contiguous().view(1, b * c, *x.size()[2:])\n",
    "\n",
    "        out = F.batch_norm(\n",
    "            x_reshaped, running_mean, running_var, self.weight, self.bias,\n",
    "            True, self.momentum, self.eps)\n",
    "\n",
    "        return out.view(b, c, *x.size()[2:])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + str(self.num_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.blocks = nn.ModuleList()\n",
    "        n_blocks = 4\n",
    "        for _ in range(n_blocks):\n",
    "            self.blocks.append(ResBlock(in_planes=in_channels, kernel_size=3,\n",
    "                                        padding=(1, \"reflection\"), norm=\"adain\"))\n",
    "        n_blocks = 2\n",
    "        self.upsample_blocks = nn.ModuleList()\n",
    "        prev_dims = 0\n",
    "        dims = 256\n",
    "        for _ in range(n_blocks):\n",
    "            prev_dims = dims\n",
    "            dims = dims // 2\n",
    "            self.upsample_blocks.append(nn.Upsample(scale_factor=2))\n",
    "            self.upsample_blocks.append(ConvNormRelu(in_channels=prev_dims, out_channels=dims,\n",
    "                                                     kernel_size=5, padding=(2, \"reflection\"), stride=1, norm=\"ln\"))\n",
    "            \n",
    "        self.last_layer = ConvNormRelu(in_channels=dims, out_channels=3, kernel_size=7,\n",
    "                                      padding=(3, \"reflection\"), stride=1, norm=None)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = inputs\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "        for block in self.upsample_blocks:\n",
    "            out = block(out)\n",
    "        return F.tanh(self.last_layer(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNormAct(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, norm='batch', activation=\"relu\"):\n",
    "        super(LinearNormAct, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(in_channels, out_channels)\n",
    "        self.norm = get_norm_module(norm)(out_channels)\n",
    "        self.leaky = leaky\n",
    "        self.activation = get_activation(name)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        out = self.fc(inputs)\n",
    "        if self.norm is not None:\n",
    "            out = self.norm(out)\n",
    "        if self.activation:\n",
    "            return self.activation(out)\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, hidden_dim, num_blocks):\n",
    "        \n",
    "        self.fc1 = LinearNormAct(in_channels=in_channels, out_channels=hidden_dim, norm=\"none\")\n",
    "        \n",
    "        self.blocks = nn.ModuleList()\n",
    "        for _ in range(num_blocks - 2):\n",
    "            self.blocks.append(LinearNormAct(in_channels=hidden_dim, out_channels=hidden_dim, norm=\"none\"))\n",
    "        \n",
    "        self.last_fc = LinearNormAct(in_channels=hidden_dim, out_channels=out_channels, norm=\"none\", activation=\"none\")\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = self.fc1(inputs)\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "        return self.last_fc(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MUnitAutoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, mlp_hidden_dim, mlp_num_blocks, enc_style_dims, enc_cont_num_blocks, enc_cont_dim=256):\n",
    "        super(MUnitAutoencoder, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.enc_cont = ContentEncoder(num_channels=in_channels, num_blocks=enc_cont_num_blocks, content_dim=enc_cont_dim)\n",
    "        self.enc_style = StyleEncoder(num_channels=in_channels, style_dims=enc_style_dims)\n",
    "        \n",
    "        self.decoder = Decoder(in_channels=enc_cont_dim)\n",
    "        self.mlp = MLP(in_channels=enc_style_dims, out_channels=get_num_adain_params,\n",
    "                       hidden_dim=mlp_hidden_dim, num_blocks=mlp_num_blocks)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def encode(self, inputs):\n",
    "        enc_cont = self.enc_cont(inputs)\n",
    "        enc_style = self.enc_style(inputs)\n",
    "        return enc_cont, enc_style\n",
    "    \n",
    "    def decode(self, enc_cont, enc_style):\n",
    "        features = self.mlp(enc_style)\n",
    "        assign_adain_params(features, self.decoder)\n",
    "        return self.decoder(enc_cont)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        enc_cont, enc_style = self.encode(inputs)\n",
    "        rec_inputs = self.decode(enc_cont, enc_style)\n",
    "        return rec_inputs\n",
    "    \n",
    "    #Nvidia\n",
    "    def assign_adain_params(self, adain_params, model):\n",
    "        # assign the adain_params to the AdaIN layers in model\n",
    "        for m in model.modules():\n",
    "            if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n",
    "                mean = adain_params[:, :m.num_features]\n",
    "                std = adain_params[:, m.num_features:2*m.num_features]\n",
    "                m.bias = mean.contiguous().view(-1)\n",
    "                m.weight = std.contiguous().view(-1)\n",
    "                if adain_params.size(1) > 2*m.num_features:\n",
    "                    adain_params = adain_params[:, 2*m.num_features:]\n",
    "    \n",
    "    #Nvidia\n",
    "    def get_num_adain_params(self, model):\n",
    "        # return the number of AdaIN parameters needed by the model\n",
    "        num_adain_params = 0\n",
    "        for m in model.modules():\n",
    "            if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n",
    "                num_adain_params += 2*m.num_features\n",
    "        return num_adain_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSDiscriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, num_scales):\n",
    "        super(MSDiscriminator, self).__init__()\n",
    "        \n",
    "        self.discrs = nn.ModuleList()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_scales = num_scales\n",
    "        for _ in range(self.num_scales):\n",
    "            self.discrs.append(self.create_discr(self.in_channels))\n",
    "        \n",
    "        self.downsample = nn.AvgPool2d(kernel_size=3, stride=2, padding=1, count_include_pad=False)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def create_discr(self, in_channels):\n",
    "        prev_dims = 0\n",
    "        dims = 64\n",
    "        self.discr = []\n",
    "        n_blocks = 3\n",
    "        \n",
    "        self.model += [ConvNormRelu(in_channels=in_channels, out_channels=dims, \n",
    "                                  kernel_size=4, padding=(1, \"zeros\"), stride=2, leaky=True, norm='none')]\n",
    "        \n",
    "        for _ in range(n_blocks):\n",
    "            prev_dims = dims\n",
    "            dims = dims * 2\n",
    "            self.model += [ConvNormRelu(in_channels=prev_dims, out_channels=dims, \n",
    "                                  kernel_size=4, padding=(1, \"zeros\"), stride=2, leaky=True, norm='instance')]\n",
    "        \n",
    "        self.model += [nn.Conv2d(dims, out_channels=1, kernel_size=1, padding=0)]\n",
    "        return nn.Sequential(*self.model)\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = []\n",
    "        for discr in self.discrs:\n",
    "            outputs.append(discr(inputs))\n",
    "            inputs = self.downsample(inputs)\n",
    "        return outputs\n",
    "    \n",
    "    def discr_loss(self, real, fake):\n",
    "        outputs_real = self.forward(real)\n",
    "        outputs_fake = self.forward(fake)\n",
    "        loss_fake = 0\n",
    "        loss_real = 0\n",
    "        for i in range(self.num_scales):\n",
    "            loss_fake += calc_mse_loss(outputs_fake[i], 1)\n",
    "            loss_real += calc_mse_loss(outputs_real[i], 0)\n",
    "        \n",
    "        loss = (loss_fake + loss_real) * (1/2)\n",
    "        #loss.backward()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def gen_loss(self, fake):\n",
    "        loss = 0\n",
    "        outputs_fake = self.forward(fake)\n",
    "        for i in range(self.num_scales):\n",
    "            loss += calc_mse_loss(outputs_fake[i], 1) \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.randn(a.shape).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        cont_A, style_A = G1.encode(inputs)\n",
    "        rec_img_A = G1.decode(cont_A, style_A)\n",
    "        \n",
    "        fake_style = torch.randn(*style.shape)\n",
    "        \n",
    "        cont_B, fake_style_B = G2.decode(cont_A, fake_style)\n",
    "        \n",
    "        \n",
    "        img_rec_loss = F.l1_loss(rec_image, inputs)\n",
    "        \n",
    "        cont_loss = F.l1_loss(cont_B, cont_A)\n",
    "        \n",
    "        style_loss = F.l1_loss(fake_style_B, fake_style)\n",
    "        \n",
    "        adv_loss = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
